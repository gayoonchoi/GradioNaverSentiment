# src/application/utils.py
import os
import pandas as pd
import re
import math
import json
import hashlib
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import traceback
from ..infrastructure.llm_client import get_llm_client # ìƒëŒ€ ê²½ë¡œ ì„í¬íŠ¸ ìˆ˜ì •

PAGE_SIZE = 10

# ìºì‹œ ì„¤ì •
CACHE_DIR = "cache"
CACHE_EXPIRY_DAYS = 30  # ìºì‹œ ë§Œë£Œ ê¸°ê°„ (ì¼)

def get_cache_key(keyword: str, num_reviews: int) -> str:
    """í‚¤ì›Œë“œì™€ ë¦¬ë·° ê°œìˆ˜ë¡œ ìºì‹œ í‚¤ ìƒì„±"""
    key_str = f"{keyword}_{num_reviews}"
    return hashlib.md5(key_str.encode('utf-8')).hexdigest()

def get_cache_path(cache_key: str) -> str:
    """ìºì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    os.makedirs(CACHE_DIR, exist_ok=True)
    return os.path.join(CACHE_DIR, f"{cache_key}.json")

def is_cache_valid(cache_path: str) -> bool:
    """ìºì‹œ íŒŒì¼ì´ ìœ íš¨í•œì§€ í™•ì¸ (ì¡´ì¬ ì—¬ë¶€ ë° ë§Œë£Œ ê¸°ê°„)"""
    if not os.path.exists(cache_path):
        return False

    # ìºì‹œ íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
    file_mtime = datetime.fromtimestamp(os.path.getmtime(cache_path))
    expiry_date = datetime.now() - timedelta(days=CACHE_EXPIRY_DAYS)

    return file_mtime > expiry_date

def load_cached_analysis(keyword: str, num_reviews: int) -> dict:
    """ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
    try:
        cache_key = get_cache_key(keyword, num_reviews)
        cache_path = get_cache_path(cache_key)

        if not is_cache_valid(cache_path):
            return None

        with open(cache_path, 'r', encoding='utf-8') as f:
            cached_data = json.load(f)
            print(f"âœ… ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©: {keyword} (num_reviews={num_reviews})")
            return cached_data
    except Exception as e:
        print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def save_analysis_to_cache(keyword: str, num_reviews: int, results: dict) -> None:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥ (API í˜•ì‹ìš©)"""
    try:
        cache_key = get_cache_key(keyword, num_reviews)
        cache_path = get_cache_path(cache_key)

        # pandas DataFrameì„ dictë¡œ ë³€í™˜í•  ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬
        cacheable_results = {}
        for key, value in results.items():
            if isinstance(value, pd.DataFrame):
                cacheable_results[key] = value.to_dict('records')
            elif isinstance(value, (datetime,)):
                cacheable_results[key] = value.isoformat() if value else None
            elif isinstance(value, (dict, list, str, int, float, bool, type(None))):
                cacheable_results[key] = value
            else:
                # ì§ë ¬í™”í•  ìˆ˜ ì—†ëŠ” íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                cacheable_results[key] = str(value)

        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cacheable_results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥: {keyword} (num_reviews={num_reviews})")
    except Exception as e:
        print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (ë¶„ì„ì€ ê³„ì† ì§„í–‰ë¨): {e}")

def save_raw_analysis_to_cache(keyword: str, num_reviews: int, results: dict) -> None:
    """ì›ë³¸ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥ (ë‚´ë¶€ ì¬ì‚¬ìš©ìš©)"""
    try:
        cache_key = get_cache_key(keyword, num_reviews) + "_raw"
        cache_path = get_cache_path(cache_key)

        # pandas DataFrameê³¼ datetimeì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
        cacheable_results = {}
        for key, value in results.items():
            if isinstance(value, pd.DataFrame):
                # DataFrameì„ records í˜•ì‹ê³¼ í•¨ê»˜ columns ì •ë³´ë„ ì €ì¥
                cacheable_results[key] = {
                    '_type': 'DataFrame',
                    'data': value.to_dict('records'),
                    'columns': list(value.columns)
                }
            elif isinstance(value, (datetime,)):
                cacheable_results[key] = {
                    '_type': 'datetime',
                    'value': value.isoformat() if value else None
                }
            elif isinstance(value, (dict, list, str, int, float, bool, type(None))):
                cacheable_results[key] = value
            else:
                # ì§ë ¬í™”í•  ìˆ˜ ì—†ëŠ” íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                cacheable_results[key] = str(value)

        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cacheable_results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ì›ë³¸ ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥: {keyword} (num_reviews={num_reviews})")
    except Exception as e:
        print(f"âš ï¸ ì›ë³¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (ë¶„ì„ì€ ê³„ì† ì§„í–‰ë¨): {e}")

def load_raw_cached_analysis(keyword: str, num_reviews: int) -> dict:
    """ìºì‹œëœ ì›ë³¸ ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
    try:
        cache_key = get_cache_key(keyword, num_reviews) + "_raw"
        cache_path = get_cache_path(cache_key)

        if not is_cache_valid(cache_path):
            return None

        with open(cache_path, 'r', encoding='utf-8') as f:
            cached_data = json.load(f)

            # DataFrameê³¼ datetime ë³µì›
            restored_results = {}
            for key, value in cached_data.items():
                if isinstance(value, dict) and value.get('_type') == 'DataFrame':
                    # DataFrame ë³µì›
                    restored_results[key] = pd.DataFrame(value['data'], columns=value['columns'])
                elif isinstance(value, dict) and value.get('_type') == 'datetime':
                    # datetime ë³µì›
                    restored_results[key] = datetime.fromisoformat(value['value']) if value['value'] else None
                else:
                    restored_results[key] = value

            print(f"âœ… ìºì‹œëœ ì›ë³¸ ë¶„ì„ ê²°ê³¼ ì‚¬ìš©: {keyword} (num_reviews={num_reviews})")
            return restored_results
    except Exception as e:
        print(f"âš ï¸ ì›ë³¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None

def change_page(full_df, page_num):
    if not isinstance(full_df, pd.DataFrame) or full_df.empty:
        return pd.DataFrame(), 1, "/ 1"
    try:
        page_num = int(page_num)
        total_rows = len(full_df)
        total_pages = math.ceil(total_rows / PAGE_SIZE) if total_rows > 0 else 1
        page_num = max(1, min(page_num, total_pages))
        start_idx = (page_num - 1) * PAGE_SIZE
        end_idx = start_idx + PAGE_SIZE
        return full_df.iloc[start_idx:end_idx], page_num, f"/ {total_pages}"
    except Exception as e:
        print(f"í˜ì´ì§€ ë³€ê²½ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return pd.DataFrame(), 1, "/ 1"

def get_season(postdate: str) -> str:
    try:
        # ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
        cleaned_date = re.sub(r'\D', '', postdate)
        if not cleaned_date or len(cleaned_date) < 6:
            return "ì •ë³´ì—†ìŒ"
        
        month = int(cleaned_date[4:6])
        if month in [3, 4, 5]: return "ë´„"
        elif month in [6, 7, 8]: return "ì—¬ë¦„"
        elif month in [9, 10, 11]: return "ê°€ì„"
        else: return "ê²¨ìš¸"
    except (ValueError, IndexError):
        return "ì •ë³´ì—†ìŒ"

def save_df_to_csv(df: pd.DataFrame, base_name: str, keyword: str) -> str:
    if df is None or df.empty: return None
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±° ê°•í™”
        sanitized_keyword = re.sub(r'[\\/*?"<>|:\s]+', '_', keyword) if keyword else "result"
        # í‚¤ì›Œë“œê°€ ë„ˆë¬´ ê¸¸ ê²½ìš° ì˜ë¼ë‚´ê¸°
        sanitized_keyword = sanitized_keyword[:50]
        csv_filepath = f"{sanitized_keyword}_{base_name}_{timestamp}.csv"
        df.to_csv(csv_filepath, index=False, encoding='utf-8-sig')
        return csv_filepath
    except Exception as e:
        print(f"CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜ ({keyword}): {e}")
        return None

def create_festinsight_table(results: dict, keyword: str) -> pd.DataFrame:
    """FestInsight_Analysis_Table í˜•ì‹ì˜ í†µí•© ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # DBì—ì„œ ì¶•ì œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        from ..infrastructure.web.tour_api_client import get_festival_details
        festival_details = get_festival_details(keyword)

        # ê¸°ë³¸ ì •ë³´ëŠ” resultsì—ì„œ ê°€ì ¸ì˜´
        trend_metrics = results.get("trend_metrics", {})

        table_data = {
            'keyword': keyword,
            'addr1': festival_details.get('addr1', 'N/A') if festival_details else 'N/A',
            'addr2': festival_details.get('addr2', 'N/A') if festival_details else 'N/A',
            'areaCode': festival_details.get('areacode', 'N/A') if festival_details else 'N/A',
            'eventStartDate': results.get('festival_start_date').strftime('%Y%m%d') if results.get('festival_start_date') else 'N/A',
            'eventEndDate': results.get('festival_end_date').strftime('%Y%m%d') if results.get('festival_end_date') else 'N/A',
            'eventPeriod': results.get('event_period', 'N/A'),
            'trend_index': trend_metrics.get('trend_index', 0),
            'sentiment_score': round(results.get('total_sentiment_score', 50.0), 2),
            'positive_ratio': round((results.get('total_pos', 0) / results.get('total_sentiment_frequency', 1)) * 100, 2) if results.get('total_sentiment_frequency', 0) > 0 else 0,
            'negative_ratio': round((results.get('total_neg', 0) / results.get('total_sentiment_frequency', 1)) * 100, 2) if results.get('total_sentiment_frequency', 0) > 0 else 0,
            'complaint_factor': 'ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ ì°¸ì¡°',  # ë³„ë„ ìš”ì•½ í…ìŠ¤íŠ¸ë¡œ ì œê³µ
            'satisfaction_delta': round(results.get('satisfaction_delta', 0), 2),
            'theme_sentiment_avg': 'N/A',  # ë‹¨ì¼ í‚¤ì›Œë“œ ë¶„ì„ì—ì„œëŠ” í•´ë‹¹ ì—†ìŒ
            'emotion_keyword_freq': str(results.get('emotion_keyword_freq', {}))
        }

        return pd.DataFrame([table_data])
    except Exception as e:
        print(f"FestInsight í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def create_festinsight_table_for_category(results: dict, category_name: str) -> pd.DataFrame:
    """ì¹´í…Œê³ ë¦¬ ë¶„ì„ ê²°ê³¼ë¥¼ FestInsight_Analysis_Table í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        festival_results = results.get('individual_festival_results_df', pd.DataFrame())

        if festival_results.empty:
            return pd.DataFrame()

        # ê¸°ì¡´ ì¶•ì œ ìš”ì•½ ë°ì´í„°í”„ë ˆì„ì„ FestInsight í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        table_data_list = []
        for _, row in festival_results.iterrows():
            table_data = {
                'keyword': row.get('ì¶•ì œëª…', 'N/A'),
                'addr1': 'N/A',
                'addr2': 'N/A',
                'areaCode': 'N/A',
                'eventStartDate': 'N/A',
                'eventEndDate': 'N/A',
                'eventPeriod': row.get('ì¶•ì œ ê¸°ê°„ (ì¼)', 'N/A'),
                'trend_index': row.get('íŠ¸ë Œë“œ ì§€ìˆ˜ (%)', 'N/A'),
                'sentiment_score': row.get('ê°ì„± ì ìˆ˜', 'N/A'),
                'positive_ratio': row.get('ê¸ì • ë¹„ìœ¨ (%)', 'N/A'),
                'negative_ratio': row.get('ë¶€ì • ë¹„ìœ¨ (%)', 'N/A'),
                'complaint_factor': row.get('ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ ìš”ì•½', 'N/A')[:100] if pd.notna(row.get('ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ ìš”ì•½')) else 'N/A',
                'satisfaction_delta': row.get('ë§Œì¡±ë„ ë³€í™”', 'N/A'),
                'theme_sentiment_avg': results.get('theme_sentiment_avg', 'N/A'),
                'emotion_keyword_freq': row.get('ì£¼ìš” ê°ì„± í‚¤ì›Œë“œ', 'N/A')
            }
            table_data_list.append(table_data)

        return pd.DataFrame(table_data_list)
    except Exception as e:
        print(f"ì¹´í…Œê³ ë¦¬ FestInsight í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()

def summarize_negative_feedback(sentences: list) -> str:
    if not sentences:
        if os.environ.get("LOG_DEBUG") == "true":
            print("[DEBUG] summarize_negative_feedback: No sentences provided, returning empty string.")
        return ""
    try:
        llm = get_llm_client(model="gemini-2.5-pro")

        if os.environ.get("LOG_DEBUG") == "true":
            print(f"[DEBUG] summarize_negative_feedback: Received {len(sentences)} sentences.")
            print(f"[DEBUG] First 3 sentences: {sentences[:3]}")

        # ë¹ˆ ë¬¸ì¥, None, ë„ˆë¬´ ì§§ì€ ë¬¸ì¥(3ì ë¯¸ë§Œ) í•„í„°ë§
        filtered_sentences = [s for s in sentences if s and isinstance(s, str) and len(s.strip()) >= 3]

        if os.environ.get("LOG_DEBUG") == "true":
            print(f"[DEBUG] After basic filtering: {len(filtered_sentences)} sentences")

        unique_sentences = sorted(list(set(filtered_sentences)), key=len, reverse=True)

        if not unique_sentences:
            if os.environ.get("LOG_DEBUG") == "true":
                print("[DEBUG] summarize_negative_feedback: No unique sentences after filtering, returning empty string.")
            # ë¶€ì • íŒì •ì€ ìˆì§€ë§Œ ë¬¸ì¥ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ìœ„í•œ ëª…ì‹œì  ë©”ì‹œì§€
            return ""

        negative_feedback_str = "\n- ".join(unique_sentences[:50])
        
        if os.environ.get("LOG_DEBUG") == "true":
            print(f"[DEBUG] summarize_negative_feedback: negative_feedback_str length: {len(negative_feedback_str)}")
            print(f"[DEBUG] negative_feedback_str (first 200 chars): {negative_feedback_str[:200]}")

        prompt = f'''[ìˆ˜ì§‘ëœ ë¶€ì •ì ì¸ ì˜ê²¬]\n- {negative_feedback_str}\n\n[ìš”ì²­] ìœ„ ì˜ê²¬ë“¤ì„ ì¢…í•©í•˜ì—¬ ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ì„ 1., 2., 3. ... í˜•ì‹ì˜ ëª©ë¡ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ë§Œì•½ ì˜ê²¬ì´ ì—†ë‹¤ë©´ 'íŠ¹ë³„í•œ ë¶ˆë§Œ ì‚¬í•­ ì—†ìŒ'ì´ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.'''
        
        if os.environ.get("LOG_DEBUG") == "true":
            print(f"[DEBUG] summarize_negative_feedback: LLM Prompt (first 500 chars): {prompt[:500]}")

        response = llm.invoke(prompt)
        
        if os.environ.get("LOG_DEBUG") == "true":
            print(f"[DEBUG] summarize_negative_feedback: LLM Response: {response.content.strip()}")

        return response.content.strip()
    except Exception as e:
        print(f"ë¶€ì •ì  ì˜ê²¬ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return "ë¶€ì •ì  ì˜ê²¬ì„ ìš”ì•½í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

def create_driver():
    """ì›¹ ë“œë¼ì´ë²„ ìƒì„±"""
    try:
        service = Service(ChromeDriverManager().install())
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        # í•„ìš”ì‹œ User-Agent ì¶”ê°€
        # chrome_options.add_argument("user-agent=...")
        return webdriver.Chrome(service=service, options=chrome_options)
    except Exception as e:
        print(f"WebDriver ìƒì„± ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        raise RuntimeError("WebDriverë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Chrome ë˜ëŠ” ChromeDriver ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.") from e

def calculate_trend_metrics(trend_df: pd.DataFrame, start_date: datetime, end_date: datetime):
    """íŠ¸ë Œë“œ ë°ì´í„°í”„ë ˆì„ê³¼ ì¶•ì œ ê¸°ê°„ì„ ë°”íƒ•ìœ¼ë¡œ íŠ¸ë Œë“œ ê´€ë ¨ ì§€í‘œë“¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if trend_df.empty or not start_date or not end_date:
        return {
            "trend_index": 0,
            "after_trend_index": 0,
            "before_avg": 0,
            "during_avg": 0,
            "after_avg": 0,
        }

    try:
        # ë‚ ì§œ íƒ€ì…ì„ pandas datetimeìœ¼ë¡œ í†µì¼
        trend_df['period'] = pd.to_datetime(trend_df['period'])
        start_date = pd.to_datetime(start_date)
        end_date = pd.to_datetime(end_date)

        # ê° ê¸°ê°„ ì •ì˜
        before_period_start = start_date - pd.Timedelta(days=30)
        after_period_end = end_date + pd.Timedelta(days=30)

        # ê¸°ê°„ë³„ ë°ì´í„° í•„í„°ë§
        before_df = trend_df[(trend_df['period'] >= before_period_start) & (trend_df['period'] < start_date)]
        during_df = trend_df[(trend_df['period'] >= start_date) & (trend_df['period'] <= end_date)]
        after_df = trend_df[(trend_df['period'] > end_date) & (trend_df['period'] <= after_period_end)]

        # ê° ê¸°ê°„ë³„ í‰ê·  ê³„ì‚°
        before_avg = before_df['ratio'].mean() if not before_df.empty else 0
        during_avg = during_df['ratio'].mean() if not during_df.empty else 0
        after_avg = after_df['ratio'].mean() if not after_df.empty else 0

        # íŠ¸ë Œë“œ ì§€ìˆ˜ ê³„ì‚°
        # ì¶•ì œ ì „ ëŒ€ë¹„ ì¶•ì œ ì¤‘ íŠ¸ë Œë“œ ë³€í™” (0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²½ìš° ë°©ì§€)
        trend_index = (during_avg / before_avg) * 100 if before_avg > 0 else (during_avg * 100 if during_avg > 0 else 0)
        # ì¶•ì œ ì¤‘ ëŒ€ë¹„ ì¶•ì œ í›„ íŠ¸ë Œë“œ ë³€í™”
        after_trend_index = (after_avg / during_avg) * 100 if during_avg > 0 else (after_avg * 100 if after_avg > 0 else 0)
        
        return {
            "trend_index": round(trend_index, 1),
            "after_trend_index": round(after_trend_index, 1),
            "before_avg": round(before_avg, 1),
            "during_avg": round(during_avg, 1),
            "after_avg": round(after_avg, 1),
        }
    except Exception as e:
        print(f"íŠ¸ë Œë“œ ì§€í‘œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return {
            "trend_index": 0, "after_trend_index": 0, "before_avg": 0, "during_avg": 0, "after_avg": 0,
        }

def calculate_satisfaction_boundaries(scores: list) -> dict:
    """
    ê°ì„± ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° IQR ê¸°ë°˜ ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³ ,
    ë§Œì¡±ë„ 5ë‹¨ê³„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê²½ê³„ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

    Returns:
        dict: {
            "boundaries": {mean, std, very_dissatisfied_upper, dissatisfied_upper, neutral_upper, satisfied_upper},
            "filtered_scores": ì´ìƒì¹˜ ì œê±°ëœ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸,
            "outliers": ì´ìƒì¹˜ ë¦¬ìŠ¤íŠ¸
        }
    """
    import numpy as np

    if not scores:
        return {
            "boundaries": {},
            "filtered_scores": [],
            "outliers": [],
        }

    q1 = np.percentile(scores, 25)
    q3 = np.percentile(scores, 75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    filtered_scores = [s for s in scores if lower_bound <= s <= upper_bound]
    outliers = [s for s in scores if s < lower_bound or s > upper_bound]

    if not filtered_scores:
        filtered_scores = scores

    mean = np.mean(filtered_scores)
    std = np.std(filtered_scores)

    # Handle case where all scores are identical
    if np.isclose(std, 0):
        std = 0.1

    boundaries = {
        "mean": mean,
        "std": std,
        "very_dissatisfied_upper": mean - 1.5 * std,
        "dissatisfied_upper": mean - 0.5 * std,
        "neutral_upper": mean + 0.5 * std,
        "satisfied_upper": mean + 1.5 * std,
    }
    return {
        "boundaries": boundaries,
        "filtered_scores": filtered_scores,
        "outliers": outliers,
    }

def map_score_to_level(score: float, boundaries: dict) -> int:
    """
    ê°ì„± ì ìˆ˜ë¥¼ 5ë‹¨ê³„ ë§Œì¡±ë„ ë ˆë²¨ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.

    Args:
        score: ê°ì„± ì ìˆ˜ (-2.0 ~ +2.0 ë²”ìœ„)
        boundaries: calculate_satisfaction_boundaries()ë¡œ ê³„ì‚°ëœ ê²½ê³„ê°’ ë”•ì…”ë„ˆë¦¬

    Returns:
        int: 1 (ë§¤ìš° ë¶ˆë§Œì¡±) ~ 5 (ë§¤ìš° ë§Œì¡±)
    """
    if not boundaries:
        return 3  # Default to neutral if no boundaries
    if score < boundaries["very_dissatisfied_upper"]:
        return 1  # ë§¤ìš° ë¶ˆë§Œì¡±
    elif score < boundaries["dissatisfied_upper"]:
        return 2  # ë¶ˆë§Œì¡±
    elif score < boundaries["neutral_upper"]:
        return 3  # ë³´í†µ
    elif score < boundaries["satisfied_upper"]:
        return 4  # ë§Œì¡±
    else:
        return 5  # ë§¤ìš° ë§Œì¡±

def generate_distribution_interpretation(satisfaction_counts: dict, total_count: int, boundaries: dict, avg_satisfaction: float,
                                        all_scores: list = None, outliers: list = None, total_pos: int = 0, total_neg: int = 0,
                                        trend_metrics: dict = None) -> str:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì°¨íŠ¸ ë°ì´í„°(6ê°œ)ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ ìì—°ì–´ í•´ì„ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        satisfaction_counts: {'ë§¤ìš° ë¶ˆë§Œì¡±': N, 'ë¶ˆë§Œì¡±': N, ...} í˜•íƒœì˜ ì¹´ìš´íŠ¸ ë”•ì…”ë„ˆë¦¬
        total_count: ì „ì²´ ë¬¸ì¥ ìˆ˜
        boundaries: ë§Œì¡±ë„ ê²½ê³„ê°’ ë”•ì…”ë„ˆë¦¬
        avg_satisfaction: í‰ê·  ë§Œì¡±ë„ (1.0 ~ 5.0)
        all_scores: ì ˆëŒ€ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜)
        outliers: ì´ìƒì¹˜ ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜)
        total_pos: ê¸ì • ë¬¸ì¥ ìˆ˜ (ì˜µì…˜)
        total_neg: ë¶€ì • ë¬¸ì¥ ìˆ˜ (ì˜µì…˜)
        trend_metrics: íŠ¸ë Œë“œ ì§€í‘œ (ì˜µì…˜)

    Returns:
        str: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì¢…í•© í•´ì„ í…ìŠ¤íŠ¸
    """
    try:
        llm = get_llm_client(model="gemini-2.5-pro")

        # ì¹´ìš´íŠ¸ì™€ ë¹„ìœ¨ ê³„ì‚°
        labels = ["ë§¤ìš° ë¶ˆë§Œì¡±", "ë¶ˆë§Œì¡±", "ë³´í†µ", "ë§Œì¡±", "ë§¤ìš° ë§Œì¡±"]
        counts_str = "\n".join([f"- {label}: {satisfaction_counts.get(label, 0)}ê°œ ({satisfaction_counts.get(label, 0) / total_count * 100:.1f}%)"
                                for label in labels])

        # 1. ì „ì²´ ê¸ì •/ë¶€ì • ë¹„ìœ¨ ë°ì´í„°
        total_sentiment = total_pos + total_neg if (total_pos or total_neg) else total_count
        pos_ratio = (total_pos / total_sentiment * 100) if total_sentiment > 0 else 0
        neg_ratio = (total_neg / total_sentiment * 100) if total_sentiment > 0 else 0

        sentiment_ratio_str = f"""
### 1. ì „ì²´ ê¸ì •/ë¶€ì • ë¹„ìœ¨
- ê¸ì • ë¬¸ì¥: {total_pos}ê°œ ({pos_ratio:.1f}%)
- ë¶€ì • ë¬¸ì¥: {total_neg}ê°œ ({neg_ratio:.1f}%)"""

        # 2. ë§Œì¡±ë„ 5ë‹¨ê³„ ë¶„í¬ ë°ì´í„°
        satisfaction_dist_str = f"""
### 2. ë§Œì¡±ë„ 5ë‹¨ê³„ ë¶„í¬
{counts_str}
- í‰ê·  ë§Œì¡±ë„: {avg_satisfaction:.2f} / 5.0"""

        # 3. ì ˆëŒ€ ì ìˆ˜ ë¶„í¬ ë°ì´í„°
        score_dist_str = ""
        if all_scores:
            import numpy as np
            min_score = np.min(all_scores)
            max_score = np.max(all_scores)
            median_score = np.median(all_scores)
            score_dist_str = f"""
### 3. ì ˆëŒ€ ì ìˆ˜ ë¶„í¬
- ìµœì†Œ ì ìˆ˜: {min_score:.2f}
- ìµœëŒ€ ì ìˆ˜: {max_score:.2f}
- ì¤‘ê°„ê°’: {median_score:.2f}
- í‰ê· : {boundaries.get('mean', 0):.2f}
- í‘œì¤€í¸ì°¨: {boundaries.get('std', 0):.2f}"""

        # 4. ì´ìƒì¹˜ ë¶„ì„ ë°ì´í„°
        outlier_str = ""
        if outliers is not None:
            outlier_ratio = (len(outliers) / total_count * 100) if total_count > 0 else 0
            outlier_str = f"""
### 4. ì´ìƒì¹˜ ë¶„ì„
- ì´ìƒì¹˜ ê°œìˆ˜: {len(outliers)}ê°œ (ì „ì²´ì˜ {outlier_ratio:.1f}%)
- í•´ì„: {'ê·¹ë‹¨ì ì¸ ì˜ê²¬ì´ ë‹¤ìˆ˜ ì¡´ì¬í•©ë‹ˆë‹¤.' if outlier_ratio > 10 else 'ëŒ€ë¶€ë¶„ì˜ ì˜ê²¬ì´ í‰ê· ì ì¸ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.'}"""

        # 5. íŠ¸ë Œë“œ ë¶„ì„ ë°ì´í„°
        trend_str = ""
        if trend_metrics:
            trend_index = trend_metrics.get('trend_index', 0)
            before_avg = trend_metrics.get('before_avg', 0)
            during_avg = trend_metrics.get('during_avg', 0)
            after_avg = trend_metrics.get('after_avg', 0)
            trend_str = f"""
### 5&6. íŠ¸ë Œë“œ ë¶„ì„ (ì§‘ì¤‘ & ì „ì²´)
- ì¶•ì œ ì „ í‰ê·  ê²€ìƒ‰ëŸ‰: {before_avg:.1f}
- ì¶•ì œ ì¤‘ í‰ê·  ê²€ìƒ‰ëŸ‰: {during_avg:.1f}
- ì¶•ì œ í›„ í‰ê·  ê²€ìƒ‰ëŸ‰: {after_avg:.1f}
- íŠ¸ë Œë“œ ì§€ìˆ˜: {trend_index:.1f}% (100 ì´ìƒì´ë©´ ì¶•ì œ ê¸°ê°„ ë™ì•ˆ ê´€ì‹¬ë„ ì¦ê°€)"""

        prompt = f"""ë‹¤ìŒì€ ì¶•ì œ ë¦¬ë·°ì˜ ì¢…í•© ë¶„ì„ ë°ì´í„°ì…ë‹ˆë‹¤. **6ê°œì˜ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬** ì „ë¬¸ê°€ ì…ì¥ì—ì„œ ì¢…í•©ì ì¸ í•´ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.

**ì „ì²´ ë¦¬ë·° ë¬¸ì¥ ìˆ˜**: {total_count}ê°œ
{sentiment_ratio_str}
{satisfaction_dist_str}
{score_dist_str}
{outlier_str}
{trend_str}

ìœ„ì˜ **6ê°€ì§€ ì°¨íŠ¸ ë°ì´í„°(ê¸ì •/ë¶€ì • ë¹„ìœ¨, ë§Œì¡±ë„ 5ë‹¨ê³„, ì ˆëŒ€ ì ìˆ˜ ë¶„í¬, ì´ìƒì¹˜ ë¶„ì„, íŠ¸ë Œë“œ ë¶„ì„)ë¥¼ ëª¨ë‘ ì¢…í•©í•˜ì—¬** ë‹¤ìŒì„ í¬í•¨í•˜ëŠ” **3-5ë¬¸ì¥**ì˜ ì¢…í•© í•´ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. ì „ë°˜ì ì¸ í‰ê°€ ê²½í–¥ (ê¸ì • vs ë¶€ì •)
2. ë§Œì¡±ë„ ë¶„í¬ì˜ íŠ¹ì§• (ì–´ëŠ êµ¬ê°„ì— ì§‘ì¤‘ë˜ì–´ ìˆëŠ”ì§€)
3. ê°ì„± ì ìˆ˜ì˜ ë¶„í¬ íŠ¹ì„± (ê·¹ë‹¨ì  vs ì¤‘ë¦½ì )
4. ì´ìƒì¹˜ ì¡´ì¬ ì—¬ë¶€ì™€ ì˜ë¯¸
5. íŠ¸ë Œë“œ ì§€ìˆ˜ë¥¼ ê³ ë ¤í•œ í™”ì œì„± vs ë§Œì¡±ë„ ê´€ê³„
6. ì¢…í•©ì ì¸ ì¶•ì œ í‰ê°€ ë° ì‹œì‚¬ì 

ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""

        response = llm.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        print(f"ì¢…í•© ì°¨íŠ¸ í•´ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return f"í‰ê·  ë§Œì¡±ë„ëŠ” {avg_satisfaction:.2f} / 5.0ì ì…ë‹ˆë‹¤. ê¸ì • ë¹„ìœ¨ì€ {(total_pos/(total_pos+total_neg)*100) if (total_pos+total_neg) > 0 else 0:.1f}%ì…ë‹ˆë‹¤."

def generate_overall_summary(results: dict) -> str:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ì¢…í•© í‰ê°€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        llm = get_llm_client(model="gemini-2.5-pro")
        
        # í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ì£¼ìš” ì§€í‘œ ì¶”ì¶œ
        keyword = results.get('keyword', 'í•´ë‹¹ ì¶•ì œ')
        avg_satisfaction = results.get('avg_satisfaction', 3.0)
        total_pos = results.get('total_pos', 0)
        total_neg = results.get('total_neg', 0)
        pos_ratio = (total_pos / (total_pos + total_neg) * 100) if (total_pos + total_neg) > 0 else 0
        trend_index = results.get('trend_metrics', {}).get('trend_index', 0)
        distribution_interpretation = results.get('distribution_interpretation', 'ë°ì´í„° ì—†ìŒ')
        negative_summary = results.get('negative_summary', 'ë°ì´í„° ì—†ìŒ')

        prompt = f"""
        **{keyword}**ì— ëŒ€í•œ ì¢…í•© ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì•„ë˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ê°€ ì…ì¥ì—ì„œ ìµœì¢… í‰ê°€ë¥¼ ë‚´ë ¤ì£¼ì„¸ìš”.

        ### ì£¼ìš” ì§€í‘œ
        - **í‰ê·  ë§Œì¡±ë„**: {avg_satisfaction:.2f} / 5.0
        - **ê¸ì •/ë¶€ì • ë¹„ìœ¨**: ê¸ì • {pos_ratio:.1f}% / ë¶€ì • {100-pos_ratio:.1f}%
        - **í™”ì œì„± (íŠ¸ë Œë“œ ì§€ìˆ˜)**: {trend_index:.1f} (100 ì´ìƒì´ë©´ í‰ê·  ì´ìƒ)
        
        ### AI ë¶„ì„ ìš”ì•½
        - **ë§Œì¡±ë„ ë¶„í¬ í•´ì„**: {distribution_interpretation}
        - **ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­**: {negative_summary}

        ### ìµœì¢… í‰ê°€ ìš”ì²­
        ìœ„ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬, ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•˜ëŠ” ìµœì¢… í‰ê°€ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        1.  **í•œì¤„ ìš”ì•½**: ì´ ì¶•ì œì˜ í•µì‹¬ì ì¸ í‰ê°€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤. (ì˜ˆ: "í™”ì œì„±ì€ ë†’ì§€ë§Œ, ì£¼ì°¨ ë¬¸ì œ ë“± ìš´ì˜ ê°œì„ ì´ í•„ìš”í•œ ì¶•ì œì…ë‹ˆë‹¤.")
        2.  **ì¢…í•© ì˜ê²¬**: ì „ë°˜ì ì¸ ê¸ì •/ë¶€ì • ì—¬ë¡ , ë§Œì¡±ë„ ìˆ˜ì¤€, í™”ì œì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ìƒì„¸ ì˜ê²¬ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
        3.  **ê°•ì  (Pros)**: ë¶„ì„ ê²°ê³¼ì—ì„œ ë‚˜íƒ€ë‚œ ê¸ì •ì ì¸ ì¸¡ë©´ì„ 1-2ê°€ì§€ í•­ëª©ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
        4.  **ê°œì„ ì  (Cons)**: 'ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­'ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ì„ 1-2ê°€ì§€ í•­ëª©ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

        ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ, ê° í•­ëª©ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        """

        response = llm.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        print(f"ì¢…í•© í‰ê°€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return "ì¢…í•© í‰ê°€ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."