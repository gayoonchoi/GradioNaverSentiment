# src/application/analysis_logic.py
import pandas as pd
import re
import traceback
import gradio as gr
import os
from datetime import datetime
from ..data import festival_loader
from .utils import get_season, summarize_negative_feedback, calculate_trend_metrics
from ..application.graph import app_llm_graph
from ..infrastructure.web.naver_api import search_naver_blog_page
from ..infrastructure.web.scraper import scrape_blog_content
from ..infrastructure.web.naver_trend_api import create_trend_graph, create_focused_trend_graph
from ..infrastructure.web.tour_api_client import get_festival_period
from collections import Counter

def analyze_single_keyword_fully(keyword: str, num_reviews: int, driver, log_details: bool, progress: gr.Progress, progress_desc: str):
    # TourAPIì—ì„œ ì¶•ì œ ê¸°ê°„ ê°€ì ¸ì˜¤ê¸°
    start_date_str, end_date_str = get_festival_period(keyword)
    event_period = None
    if start_date_str and end_date_str:
        try:
            start_date = datetime.strptime(start_date_str, "%Y%m%d")
            end_date = datetime.strptime(end_date_str, "%Y%m%d")
            event_period = (end_date - start_date).days + 1
        except ValueError:
            start_date, end_date = None, None
    else:
        start_date, end_date = None, None

    search_keyword = f"{keyword} í›„ê¸°"
    max_candidates = max(50, num_reviews * 10)
    candidate_blogs, blog_results_list, all_negative_sentences = [], [], []
    blog_judgments_list = []
    emotion_keywords = []
    all_scores = []  # ë§Œì¡±ë„ ê³„ì‚°ì„ ìœ„í•œ ì „ì²´ ì ìˆ˜ ìˆ˜ì§‘
    seasonal_aspect_pairs = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
    seasonal_texts = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
    total_pos, total_neg, total_searched, start_index = 0, 0, 0, 1
    total_strong_pos, total_strong_neg = 0, 0
    seasonal_data = {"ë´„": {"pos": 0, "neg": 0}, "ì—¬ë¦„": {"pos": 0, "neg": 0}, "ê°€ì„": {"pos": 0, "neg": 0}, "ê²¨ìš¸": {"pos": 0, "neg": 0}, "ì •ë³´ì—†ìŒ": {"pos": 0, "neg": 0}}

    try:
        max_api_calls = 10
        for call_num in range(max_api_calls):
            if len(candidate_blogs) >= max_candidates: break
            progress((call_num + 1) / max_api_calls * 0.2, desc=f"[{progress_desc}] {keyword} ë¸”ë¡œê·¸ í›„ë³´ ìˆ˜ì§‘ ì¤‘... ({len(candidate_blogs)}/{max_candidates})")
            api_results = search_naver_blog_page(search_keyword, start_index=start_index)
            if not api_results: break
            total_searched += len(api_results)
            for item in api_results:
                if "blog.naver.com" in item["link"]:
                    item['title'] = re.sub(r'<[^>]+>', '', item['title']).strip()
                    if item['title'] and item["link"]:
                        candidate_blogs.append(item)
                    if len(candidate_blogs) >= max_candidates: break
            start_index += 100
            if start_index > 901: break
    except Exception as e:
        print(f"ë¸”ë¡œê·¸ í›„ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ({keyword}): {e}")
        traceback.print_exc()
        return {"error": f"'{search_keyword}' ë¸”ë¡œê·¸ í›„ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}

    if not candidate_blogs: return {"error": f"'{search_keyword}'ì— ëŒ€í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    valid_blogs_data = []
    num_candidates_to_process = len(candidate_blogs)
    initial_progress = 0.2
    
    for i, blog_data in enumerate(candidate_blogs):
        if len(valid_blogs_data) >= num_reviews: break
        progress(initial_progress + (i + 1) / num_candidates_to_process * 0.8, desc=f"[{progress_desc}] {keyword} ë¶„ì„ ì¤‘... ({len(valid_blogs_data)}/{num_reviews} ì™„ë£Œ, {i+1}/{num_candidates_to_process} í™•ì¸)")

        try:
            content = scrape_blog_content(driver, blog_data["link"])
            if not content or "ì˜¤ë¥˜" in content or "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in content: continue

            max_content_length = 30000
            if len(content) > max_content_length:
                content = content[:max_content_length] + "... (ë‚´ìš© ì¼ë¶€ ìƒëµ)"

            final_state = app_llm_graph.invoke({
                "original_text": content, "keyword": keyword, "title": blog_data["title"], 
                "log_details": log_details, "re_summarize_count": 0, "is_relevant": False
            })

            if not final_state or not final_state.get("is_relevant"): continue

            judgments = final_state.get("final_judgments", [])
            if not judgments: continue
            
            for j in judgments:
                if j.get("sentiment_keyword"):
                    emotion_keywords.append(j["sentiment_keyword"])

            season = get_season(blog_data.get('postdate', ''))
            seasonal_texts[season].append(content)
            
            aspect_pairs = final_state.get("aspect_sentiment_pairs", [])
            if aspect_pairs:
                seasonal_aspect_pairs[season].extend(aspect_pairs)

            # ì ìˆ˜ ìˆ˜ì§‘ (ë§Œì¡±ë„ ê³„ì‚°ìš©)
            for j in judgments:
                if "score" in j:
                    all_scores.append(j["score"])

            blog_judgments_list.append(judgments)
            pos_count = sum(1 for res in judgments if res["final_verdict"] == "ê¸ì •")
            neg_count = sum(1 for res in judgments if res["final_verdict"] == "ë¶€ì •")

            strong_pos_count = sum(1 for res in judgments if res["final_verdict"] == "ê¸ì •" and res["score"] >= 1.0)
            strong_neg_count = sum(1 for res in judgments if res["final_verdict"] == "ë¶€ì •" and res["score"] < -1.0)

            total_pos += pos_count
            total_neg += neg_count
            total_strong_pos += strong_pos_count
            total_strong_neg += strong_neg_count
            all_negative_sentences.extend([res["sentence"] for res in judgments if res["final_verdict"] == "ë¶€ì •"])
            
            seasonal_data[season]["pos"] += pos_count
            seasonal_data[season]["neg"] += neg_count
            
            sentiment_frequency = pos_count + neg_count
            sentiment_score = ((strong_pos_count - strong_neg_count) / sentiment_frequency * 50 + 50) if sentiment_frequency > 0 else 50.0
            pos_perc = (pos_count/sentiment_frequency*100) if sentiment_frequency > 0 else 0.0
            neg_perc = (neg_count/sentiment_frequency*100) if sentiment_frequency > 0 else 0.0

            blog_results_list.append({
                "ë¸”ë¡œê·¸ ì œëª©": blog_data["title"], "ë§í¬": blog_data["link"], "ê°ì„± ë¹ˆë„": sentiment_frequency, 
                "ê°ì„± ì ìˆ˜": f"{sentiment_score:.1f}", "ê¸ì • ë¬¸ì¥ ìˆ˜": pos_count, "ë¶€ì • ë¬¸ì¥ ìˆ˜": neg_count,
                "ê¸ì • ë¹„ìœ¨ (%)": f"{pos_perc:.1f}", "ë¶€ì • ë¹„ìœ¨ (%)": f"{neg_perc:.1f}",
                "ê¸/ë¶€ì • ë¬¸ì¥ ìš”ì•½": "\n---\n".join([f"[{res['final_verdict']}] {res['sentence']}" for res in judgments])
            })
            valid_blogs_data.append(blog_data)
        except Exception as e:
            print(f"ë¸”ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ({keyword}, {blog_data.get('link', 'N/A')}): {e}")
            traceback.print_exc()
            continue

    if not valid_blogs_data: return {"error": f"'{keyword}'ì— ëŒ€í•œ ìœ íš¨í•œ í›„ê¸° ë¸”ë¡œê·¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (í›„ë³´ {len(candidate_blogs)}ê°œ í™•ì¸)."}

    # ë§Œì¡±ë„ 5ë‹¨ê³„ ë¶„ë¥˜ ê³„ì‚°
    from .utils import calculate_satisfaction_boundaries, map_score_to_level, generate_distribution_interpretation
    import numpy as np

    boundary_results = calculate_satisfaction_boundaries(all_scores)
    boundaries = boundary_results["boundaries"]
    outliers = boundary_results["outliers"]

    # ê° judgmentì— ë§Œì¡±ë„ ë ˆë²¨ ì¶”ê°€
    all_satisfaction_levels = []
    level_map = {1: "ë§¤ìš° ë¶ˆë§Œì¡±", 2: "ë¶ˆë§Œì¡±", 3: "ë³´í†µ", 4: "ë§Œì¡±", 5: "ë§¤ìš° ë§Œì¡±"}

    for i, judgments in enumerate(blog_judgments_list):
        for j in judgments:
            if "score" in j:
                level = map_score_to_level(j["score"], boundaries)
                j["satisfaction_level"] = level
                all_satisfaction_levels.append(level)

    # ë§Œì¡±ë„ ì¹´ìš´íŠ¸ ì§‘ê³„
    satisfaction_counts = Counter([level_map.get(level, "ë³´í†µ") for level in all_satisfaction_levels])
    avg_satisfaction = np.mean(all_satisfaction_levels) if all_satisfaction_levels else 3.0

    # LLMì„ ì‚¬ìš©í•œ ë¶„í¬ í•´ì„ ìƒì„±
    distribution_interpretation = ""
    if all_satisfaction_levels:
        try:
            distribution_interpretation = generate_distribution_interpretation(
                satisfaction_counts, len(all_satisfaction_levels), boundaries, avg_satisfaction
            )
        except Exception as e:
            print(f"ë§Œì¡±ë„ ë¶„í¬ í•´ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            distribution_interpretation = f"í‰ê·  ë§Œì¡±ë„: {avg_satisfaction:.2f} / 5.0"

    # blog_results_list ì—…ë°ì´íŠ¸ (ë§Œì¡±ë„ ë ˆë²¨ í¬í•¨)
    for i, blog_result in enumerate(blog_results_list):
        if i < len(blog_judgments_list):
            judgments = blog_judgments_list[i]
            # ë¬¸ì¥ ìš”ì•½ì— ë§Œì¡±ë„ ë ˆë²¨ ì¶”ê°€
            blog_result["ê¸/ë¶€ì • ë¬¸ì¥ ìš”ì•½"] = "\n---\n".join([
                f"[{res['final_verdict']}({res.get('satisfaction_level', 3)}ì )] {res['sentence']}"
                for res in judgments
            ])
            # í‰ê·  ë§Œì¡±ë„ ì¶”ê°€
            blog_satisfaction_levels = [j.get("satisfaction_level", 3) for j in judgments]
            blog_result["í‰ê·  ë§Œì¡±ë„"] = f"{np.mean(blog_satisfaction_levels):.2f} / 5" if blog_satisfaction_levels else "N/A"

    total_sentiment_frequency = total_pos + total_neg
    total_sentiment_score = ((total_strong_pos - total_strong_neg) / total_sentiment_frequency * 50 + 50) if total_sentiment_frequency > 0 else 50.0

    # ì§€ë‚œ 1ë…„ íŠ¸ë Œë“œ ê·¸ë˜í”„
    trend_graph_path, trend_df = create_trend_graph(keyword, festival_start_date=start_date, festival_end_date=end_date)

    # ì§‘ì¤‘ íŠ¸ë Œë“œ ê·¸ë˜í”„ (ì¶•ì œ ê¸°ê°„ì´ ì—†ì–´ë„ ìµœê·¼ 60ì¼ íŠ¸ë Œë“œ ìƒì„±)
    print(f"ğŸ” '{keyword}' ì§‘ì¤‘ íŠ¸ë Œë“œ ê·¸ë˜í”„ ìƒì„± ì‹œì‘...")
    focused_trend_graph_path, focused_trend_df = create_focused_trend_graph(keyword, start_date, end_date)

    if focused_trend_graph_path:
        print(f"âœ… '{keyword}' ì§‘ì¤‘ íŠ¸ë Œë“œ ê·¸ë˜í”„ ìƒì„± ì„±ê³µ")
    else:
        print(f"âš ï¸ '{keyword}' ì§‘ì¤‘ íŠ¸ë Œë“œ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨")

    trend_metrics = calculate_trend_metrics(trend_df, start_date, end_date)
    # satisfaction_delta: íŠ¸ë Œë“œ ëŒ€ë¹„ ê°ì„± ì¦ê° ë¹„ìœ¨
    # (ê°ì„±ì ìˆ˜ - ì¤‘ë¦½ê°’) / íŠ¸ë Œë“œì§€ìˆ˜ * 100ìœ¼ë¡œ ê³„ì‚°
    trend_index = trend_metrics.get("trend_index", 0)
    satisfaction_delta = ((total_sentiment_score - 50) / (trend_index + 1e-6)) * 100 if trend_index > 0 else (total_sentiment_score - 50)

    emotion_keyword_freq = Counter(emotion_keywords)

    return {
        "status": f"ì´ {total_searched}ê°œ ê²€ìƒ‰, {len(candidate_blogs)}ê°œ í›„ë³´ ì¤‘ {len(valid_blogs_data)}ê°œ ë¸”ë¡œê·¸ ìµœì¢… ë¶„ì„ ì™„ë£Œ.",
        "total_pos": total_pos, "total_neg": total_neg, "total_strong_pos": total_strong_pos, "total_strong_neg": total_strong_neg,
        "total_sentiment_frequency": total_sentiment_frequency, "total_sentiment_score": total_sentiment_score,
        "seasonal_data": seasonal_data,
        "negative_sentences": all_negative_sentences,
        "blog_results_df": pd.DataFrame(blog_results_list) if blog_results_list else pd.DataFrame(),
        "blog_judgments": blog_judgments_list,
        "url_markdown": f"### ë¶„ì„ëœ ë¸”ë¡œê·¸ URL ({len(valid_blogs_data)}ê°œ)\n" + "\n".join([f"- [{b['title']}]({b['link']})" for b in valid_blogs_data]),
        "trend_graph": trend_graph_path,
        "focused_trend_graph": focused_trend_graph_path,
        "trend_df": trend_df,
        "focused_trend_df": focused_trend_df,
        "event_period": event_period,
        "festival_start_date": start_date,
        "festival_end_date": end_date,
        "trend_metrics": trend_metrics,
        "satisfaction_delta": satisfaction_delta,
        "emotion_keyword_freq": dict(emotion_keyword_freq.most_common(20)), # ìƒìœ„ 20ê°œë§Œ
        # ë§Œì¡±ë„ ë¶„ì„ ë°ì´í„° ì¶”ê°€
        "all_scores": all_scores,
        "satisfaction_counts": dict(satisfaction_counts),
        "avg_satisfaction": avg_satisfaction,
        "distribution_interpretation": distribution_interpretation,
        "satisfaction_boundaries": boundaries,
        "outliers": outliers,
        "seasonal_texts": {k: "\n".join(v) for k, v in seasonal_texts.items()},
        "seasonal_aspect_pairs": seasonal_aspect_pairs
    }

# í•µì‹¬ ë¶„ì„ ë¡œì§ì„ ë‹´ëŠ” ìƒˆ í•¨ìˆ˜
from ..infrastructure.reporting.wordclouds import create_seasonal_trend_wordcloud
from ..application.result_packager import SEASON_EN_MAP

def perform_festival_group_analysis(festivals_to_analyze: list, group_name: str, num_reviews: int, driver, log_details: bool, progress: gr.Progress, initial_progress: float, total_steps: int):
    if not festivals_to_analyze: return {"error": f"'{group_name}' ê·¸ë£¹ì—ì„œ ë¶„ì„í•  ì¶•ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    category_results, all_blog_posts_list = [], []
    festival_full_results = []
    agg_pos, agg_neg = 0, 0
    agg_strong_pos, agg_strong_neg = 0, 0
    agg_seasonal = {"ë´„": {"pos": 0, "neg": 0}, "ì—¬ë¦„": {"pos": 0, "neg": 0}, "ê°€ì„": {"pos": 0, "neg": 0}, "ê²¨ìš¸": {"pos": 0, "neg": 0}, "ì •ë³´ì—†ìŒ": {"pos": 0, "neg": 0}}
    agg_negative_sentences = []
    agg_seasonal_texts = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
    agg_seasonal_aspect_pairs = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
    seasonal_trend_scores = {"ë´„": {}, "ì—¬ë¦„": {}, "ê°€ì„": {}, "ê²¨ìš¸": {}}
    total_festivals_sentiment_score = 0
    analyzed_festivals_count = 0

    total_festivals = len(festivals_to_analyze)
    for i, festival_name in enumerate(festivals_to_analyze):
        def nested_progress_callback(p, desc=""):
            progress(initial_progress + (i + p) / total_festivals / total_steps, desc=f"ë¶„ì„ ì¤‘: {festival_name} ({i+1}/{total_festivals}) - {desc}")

        result = analyze_single_keyword_fully(festival_name, num_reviews, driver, log_details, nested_progress_callback, "ê·¸ë£¹ ë¶„ì„")
        
        if "error" in result or result.get("blog_results_df", pd.DataFrame()).empty:
            print(f"   [{festival_name}] ë¶„ì„ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ.")
            continue

        result['precomputed_negative_summary'] = summarize_negative_feedback(result.get("negative_sentences", []))
        festival_full_results.append(result)

        # ì‹œì¦Œë³„ íŠ¸ë Œë“œ ì ìˆ˜ ì§‘ê³„
        festival_start_date = result.get("festival_start_date")
        trend_df = result.get("trend_df")
        if festival_start_date and not trend_df.empty:
            season = get_season(festival_start_date.strftime("%Y%m%d"))
            if season != "ì •ë³´ì—†ìŒ":
                peak_trend_score = trend_df['ratio'].max()
                seasonal_trend_scores[season][festival_name] = peak_trend_score

        agg_pos += result.get("total_pos", 0)
        agg_neg += result.get("total_neg", 0)
        agg_strong_pos += result.get("total_strong_pos", 0)
        agg_strong_neg += result.get("total_strong_neg", 0)
        agg_negative_sentences.extend(result.get("negative_sentences", []))
        
        for season, pairs in result.get("seasonal_aspect_pairs", {}).items():
            if pairs:
                agg_seasonal_aspect_pairs[season].extend(pairs)

        for season, texts in result.get("seasonal_texts", {}).items():
            if texts: agg_seasonal_texts[season].append(texts)
        for season, data in result.get("seasonal_data", {}).items():
            agg_seasonal[season]["pos"] += data.get("pos", 0)
            agg_seasonal[season]["neg"] += data.get("neg", 0)
        
        total_freq = result.get("total_sentiment_frequency", 0)
        sentiment_score = result.get('total_sentiment_score', 50.0)
        pos_perc = (result.get('total_pos', 0) / total_freq * 100) if total_freq > 0 else 0.0
        neg_perc = (result.get('total_neg', 0) / total_freq * 100) if total_freq > 0 else 0.0
        
        total_festivals_sentiment_score += sentiment_score
        analyzed_festivals_count += 1
        
        trend_metrics = result.get("trend_metrics", {})
        
        category_results.append({
            "ì¶•ì œëª…": festival_name,
            'ê°ì„± ë¹ˆë„': total_freq,
            'ê°ì„± ì ìˆ˜': f"{sentiment_score:.1f}",
            "ê¸ì • ë¬¸ì¥ ìˆ˜": result.get("total_pos", 0),
            "ë¶€ì • ë¬¸ì¥ ìˆ˜": result.get("total_neg", 0),
            "ê¸ì • ë¹„ìœ¨ (%)": f"{pos_perc:.1f}",
            "ë¶€ì • ë¹„ìœ¨ (%)": f"{neg_perc:.1f}",
            'ì¶•ì œ ê¸°ê°„ (ì¼)': result.get('event_period', 'N/A'),
            'íŠ¸ë Œë“œ ì§€ìˆ˜ (%)': trend_metrics.get('trend_index', 'N/A'),
            'ë§Œì¡±ë„ ë³€í™”': f"{result.get('satisfaction_delta', 0):.1f}",
            'ì£¼ìš” ê°ì„± í‚¤ì›Œë“œ': str(result.get('emotion_keyword_freq', {})),
            'ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ ìš”ì•½': result['precomputed_negative_summary'] or "ì—†ìŒ"
        })

        if "blog_results_df" in result and not result["blog_results_df"].empty:
            all_blog_posts_list.append(result["blog_results_df"])

    if not category_results: 
        return {"error": f"ì„ íƒëœ ê·¸ë£¹ '{group_name}' ë‚´ ëª¨ë“  ì¶•ì œ({total_festivals}ê°œ)ì—ì„œ ìœ íš¨í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

    # ê³„ì ˆë³„ íŠ¸ë Œë“œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    seasonal_trend_wc_paths = {}
    for season, scores in seasonal_trend_scores.items():
        if scores:
            mask_path = os.path.abspath(os.path.join("assets", f"mask_{SEASON_EN_MAP.get(season)}.png"))
            wc_path = create_seasonal_trend_wordcloud(scores, f"{group_name}_{season}_íŠ¸ë Œë“œ", mask_path)
            seasonal_trend_wc_paths[season] = wc_path

    total_sentiment_frequency = agg_pos + agg_neg
    total_sentiment_score = ((agg_strong_pos - agg_strong_neg) / total_sentiment_frequency * 50 + 50) if total_sentiment_frequency > 0 else 50.0
    theme_sentiment_avg = total_festivals_sentiment_score / analyzed_festivals_count if analyzed_festivals_count > 0 else 0.0

    final_category_df = pd.DataFrame(category_results)
    final_all_blogs_df = pd.concat(all_blog_posts_list, ignore_index=True) if all_blog_posts_list else pd.DataFrame()

    return {
        "status": f"ì´ {total_festivals}ê°œ ì¶•ì œ ìš”ì²­ ì¤‘ {len(category_results)}ê°œ ë¶„ì„ ì™„ë£Œ.",
        "total_pos": agg_pos, "total_neg": agg_neg, 
        "total_sentiment_frequency": total_sentiment_frequency,
        "total_sentiment_score": total_sentiment_score,
        "theme_sentiment_avg": theme_sentiment_avg,
        "seasonal_data": agg_seasonal,
        "negative_sentences": agg_negative_sentences,
        "seasonal_texts": {k: "\n".join(v) for k, v in agg_seasonal_texts.items()},
        "seasonal_aspect_pairs": agg_seasonal_aspect_pairs,
        "seasonal_trend_wc_paths": seasonal_trend_wc_paths, # ì›Œë“œí´ë¼ìš°ë“œ ê²½ë¡œ ì¶”ê°€
        "individual_festival_results_df": final_category_df,
        "all_blog_posts_df": final_all_blogs_df,
        "festival_full_results": festival_full_results, 
        "all_blog_judgments": [j for res in festival_full_results for j in res.get("blog_judgments", [])]
    }

# ê¸°ì¡´ í•¨ìˆ˜ëŠ” ìƒˆë¡œ ë§Œë“  ê·¸ë£¹ ë¶„ì„ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ë˜í¼(wrapper)ê°€ ë¨
def perform_category_analysis(cat1, cat2, cat3, num_reviews, driver, log_details, progress: gr.Progress, initial_progress, total_steps):
    festivals_to_analyze = festival_loader.get_festivals(cat1, cat2, cat3)
    category_name = cat3 or cat2 or cat1
    return perform_festival_group_analysis(festivals_to_analyze, category_name, num_reviews, driver, log_details, progress, initial_progress, total_steps)