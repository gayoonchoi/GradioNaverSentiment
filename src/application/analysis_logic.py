# src/application/analysis_logic.py
import pandas as pd
import re
import traceback
import gradio as gr
import os
from datetime import datetime
from ..data import festival_loader
from .utils import get_season, summarize_negative_feedback, calculate_trend_metrics, generate_overall_summary, load_cached_analysis, save_analysis_to_cache, load_raw_cached_analysis, save_raw_analysis_to_cache
from ..application.graph import app_llm_graph
from ..infrastructure.web.naver_api import search_naver_blog_page
from ..infrastructure.web.scraper import scrape_blog_content
from ..infrastructure.web.naver_trend_api import create_trend_graph, create_focused_trend_graph
from ..infrastructure.web.tour_api_client import get_festival_period
from ..infrastructure.reporting.wordclouds import create_sentiment_wordclouds
from collections import Counter
from src.domain.knowledge_base import knowledge_base

# ê³„ì ˆ ì˜ë¬¸ ë§¤í•‘
SEASON_EN_MAP = {
    "ë´„": "spring",
    "ì—¬ë¦„": "summer",
    "ê°€ì„": "fall",
    "ê²¨ìš¸": "winter"
}

def analyze_single_keyword_fully(keyword: str, num_reviews: int, driver, log_details: bool, progress: gr.Progress, progress_desc: str):
    # ìºì‹œ í™•ì¸
    cached_result = load_raw_cached_analysis(keyword, num_reviews)
    if cached_result:
        return cached_result

    # TourAPIì—ì„œ ì¶•ì œ ê¸°ê°„ ê°€ì ¸ì˜¤ê¸°
    start_date_str, end_date_str = get_festival_period(keyword)
    event_period = None
    if start_date_str and end_date_str:
        try:
            start_date = datetime.strptime(start_date_str, "%Y%m%d")
            end_date = datetime.strptime(end_date_str, "%Y%m%d")
            event_period = (end_date - start_date).days + 1
        except ValueError:
            start_date, end_date = None, None
    else:
        start_date, end_date = None, None

    # TourAPIì—ì„œ ì¶•ì œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    from ..infrastructure.web.tour_api_client import get_festival_details
    festival_details = get_festival_details(keyword)
    addr1 = festival_details.get('addr1') if festival_details else None
    addr2 = festival_details.get('addr2') if festival_details else None
    areaCode = festival_details.get('areacode') if festival_details else None


    search_keyword = f"{keyword} í›„ê¸°"
    max_candidates = max(50, num_reviews * 10)
    candidate_blogs, blog_results_list, all_negative_sentences = [], [], []
    blog_judgments_list = []
    emotion_keywords = []
    all_scores = []  # ë§Œì¡±ë„ ê³„ì‚°ì„ ìœ„í•œ ì „ì²´ ì ìˆ˜ ìˆ˜ì§‘
    seasonal_aspect_pairs = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
    seasonal_texts = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
    total_pos, total_neg, total_searched, start_index = 0, 0, 0, 1
    total_strong_pos, total_strong_neg = 0, 0
    seasonal_data = {"ë´„": {"pos": 0, "neg": 0}, "ì—¬ë¦„": {"pos": 0, "neg": 0}, "ê°€ì„": {"pos": 0, "neg": 0}, "ê²¨ìš¸": {"pos": 0, "neg": 0}, "ì •ë³´ì—†ìŒ": {"pos": 0, "neg": 0}}

    try:
        max_api_calls = 10
        for call_num in range(max_api_calls):
            if len(candidate_blogs) >= max_candidates: break
            progress((call_num + 1) / max_api_calls * 0.2, desc=f"[{progress_desc}] {keyword} ë¸”ë¡œê·¸ í›„ë³´ ìˆ˜ì§‘ ì¤‘... ({len(candidate_blogs)}/{max_candidates})")
            api_results = search_naver_blog_page(search_keyword, start_index=start_index)
            if not api_results: break
            total_searched += len(api_results)
            for item in api_results:
                if "blog.naver.com" in item["link"]:
                    item['title'] = re.sub(r'<[^>]+>', '', item['title']).strip()
                    if item['title'] and item["link"]:
                        candidate_blogs.append(item)
                    if len(candidate_blogs) >= max_candidates: break
            start_index += 100
            if start_index > 901: break
    except Exception as e:
        print(f"ë¸”ë¡œê·¸ í›„ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ({keyword}): {e}")
        traceback.print_exc()
        return {"error": f"'{search_keyword}' ë¸”ë¡œê·¸ í›„ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}

    if not candidate_blogs: return {"error": f"'{search_keyword}'ì— ëŒ€í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    valid_blogs_data = []
    num_candidates_to_process = len(candidate_blogs)
    initial_progress = 0.2

    # ì—°ì† ê²€ì¦ ì‹¤íŒ¨ ì¹´ìš´í„° ì¶”ê°€
    consecutive_failures = 0
    max_consecutive_failures = 15  # ì—°ì†ìœ¼ë¡œ 15ë²ˆ ì‹¤íŒ¨í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ

    for i, blog_data in enumerate(candidate_blogs):
        if len(valid_blogs_data) >= num_reviews: break

        # ì—°ì† ì‹¤íŒ¨ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
        if consecutive_failures >= max_consecutive_failures:
            print(f"âš ï¸ [{keyword}] ì—°ì† {max_consecutive_failures}ë²ˆ ê²€ì¦ ì‹¤íŒ¨ë¡œ ë¶„ì„ ì¤‘ë‹¨. (ìœ íš¨ ë¸”ë¡œê·¸ {len(valid_blogs_data)}ê°œ ìˆ˜ì§‘)")
            break

        progress(initial_progress + (i + 1) / num_candidates_to_process * 0.8, desc=f"[{progress_desc}] {keyword} ë¶„ì„ ì¤‘... ({len(valid_blogs_data)}/{num_reviews} ì™„ë£Œ, {i+1}/{num_candidates_to_process} í™•ì¸)")

        try:
            content = scrape_blog_content(driver, blog_data["link"])
            if not content or "ì˜¤ë¥˜" in content or "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in content:
                consecutive_failures += 1
                continue

            max_content_length = 30000
            if len(content) > max_content_length:
                content = content[:max_content_length] + "... (ë‚´ìš© ì¼ë¶€ ìƒëµ)"

            final_state = app_llm_graph.invoke({
                "original_text": content, "keyword": keyword, "title": blog_data["title"],
                "log_details": log_details, "re_summarize_count": 0, "is_relevant": False
            })

            if not final_state or not final_state.get("is_relevant"):
                consecutive_failures += 1
                continue

            # ê²€ì¦ ì„±ê³µ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹
            consecutive_failures = 0

            judgments = final_state.get("final_judgments", [])
            if not judgments:
                consecutive_failures += 1  # íŒì • ê²°ê³¼ê°€ ì—†ì–´ë„ ì‹¤íŒ¨ë¡œ ê°„ì£¼
                continue

            # ê°ì„±ì–´ ì¶”ì¶œ ë¡œì§ ë³´ê°•
            all_sentiment_words = set(knowledge_base.adjectives.keys()) | \
                                  set(knowledge_base.adverbs.keys()) | \
                                  set(knowledge_base.sentiment_nouns.keys()) | \
                                  set(knowledge_base.idioms.keys())

            for j in judgments:
                keyword_found = False
                if j.get("sentiment_keyword"):
                    emotion_keywords.append(j["sentiment_keyword"])
                    keyword_found = True
                
                # LLMì´ í‚¤ì›Œë“œë¥¼ ëª»ì°¾ì•˜ì„ ê²½ìš°, ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì¬íƒìƒ‰
                if not keyword_found and j.get("final_verdict") in ["ê¸ì •", "ë¶€ì •"]:
                    sentence_words = j.get("sentence", "").split()
                    for word in sentence_words:
                        if word in all_sentiment_words:
                            emotion_keywords.append(word)
                            break # ë¬¸ì¥ ë‹¹ ì²«ë²ˆì§¸ í‚¤ì›Œë“œë§Œ ì¶”ê°€
            
            season = get_season(blog_data.get('postdate', ''))
            seasonal_texts[season].append(content)
            
            aspect_pairs = final_state.get("aspect_sentiment_pairs", [])
            if aspect_pairs:
                seasonal_aspect_pairs[season].extend(aspect_pairs)

            # ì ìˆ˜ ìˆ˜ì§‘ (ë§Œì¡±ë„ ê³„ì‚°ìš©)
            for j in judgments:
                if "score" in j:
                    all_scores.append(j["score"])

            blog_judgments_list.append(judgments)
            pos_count = sum(1 for res in judgments if res["final_verdict"] == "ê¸ì •")
            neg_count = sum(1 for res in judgments if res["final_verdict"] == "ë¶€ì •")

            strong_pos_count = sum(1 for res in judgments if res["final_verdict"] == "ê¸ì •" and res["score"] >= 1.0)
            strong_neg_count = sum(1 for res in judgments if res["final_verdict"] == "ë¶€ì •" and res["score"] < -1.0)

            total_pos += pos_count
            total_neg += neg_count
            total_strong_pos += strong_pos_count
            total_strong_neg += strong_neg_count
            all_negative_sentences.extend([res["sentence"] for res in judgments if res["final_verdict"] == "ë¶€ì •"])
            
            seasonal_data[season]["pos"] += pos_count
            seasonal_data[season]["neg"] += neg_count
            
            sentiment_frequency = pos_count + neg_count
            sentiment_score = ((strong_pos_count - strong_neg_count) / sentiment_frequency * 50 + 50) if sentiment_frequency > 0 else 50.0
            pos_perc = (pos_count/sentiment_frequency*100) if sentiment_frequency > 0 else 0.0
            neg_perc = (neg_count/sentiment_frequency*100) if sentiment_frequency > 0 else 0.0

            blog_results_list.append({
                "ë¸”ë¡œê·¸ ì œëª©": blog_data["title"], "ë§í¬": blog_data["link"], "ê°ì„± ë¹ˆë„": sentiment_frequency, 
                "ê°ì„± ì ìˆ˜": f"{sentiment_score:.1f}", "ê¸ì • ë¬¸ì¥ ìˆ˜": pos_count, "ë¶€ì • ë¬¸ì¥ ìˆ˜": neg_count,
                "ê¸ì • ë¹„ìœ¨ (%)": f"{pos_perc:.1f}", "ë¶€ì • ë¹„ìœ¨ (%)": f"{neg_perc:.1f}",
                "ê¸/ë¶€ì • ë¬¸ì¥ ìš”ì•½": "\n---\n".join([f"[{res['final_verdict']}] {res['sentence']}" for res in judgments]),
                "judgments": judgments # ì›ë³¸ judgments ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
            })
            valid_blogs_data.append(blog_data)
        except Exception as e:
            print(f"ë¸”ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ({keyword}, {blog_data.get('link', 'N/A')}): {e}")
            traceback.print_exc()
            consecutive_failures += 1  # ì˜ˆì™¸ ë°œìƒë„ ì‹¤íŒ¨ë¡œ ê°„ì£¼
            continue

    if not valid_blogs_data: return {"error": f"'{keyword}'ì— ëŒ€í•œ ìœ íš¨í•œ í›„ê¸° ë¸”ë¡œê·¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (í›„ë³´ {len(candidate_blogs)}ê°œ í™•ì¸)."}

    # ë§Œì¡±ë„ 5ë‹¨ê³„ ë¶„ë¥˜ ê³„ì‚°
    from .utils import calculate_satisfaction_boundaries, map_score_to_level, generate_distribution_interpretation
    import numpy as np

    boundary_results = calculate_satisfaction_boundaries(all_scores)
    boundaries = boundary_results["boundaries"]
    outliers = boundary_results["outliers"]

    # ê° judgmentì— ë§Œì¡±ë„ ë ˆë²¨ ì¶”ê°€
    all_satisfaction_levels = []
    level_map = {1: "ë§¤ìš° ë¶ˆë§Œì¡±", 2: "ë¶ˆë§Œì¡±", 3: "ë³´í†µ", 4: "ë§Œì¡±", 5: "ë§¤ìš° ë§Œì¡±"}

    for i, judgments in enumerate(blog_judgments_list):
        for j in judgments:
            if "score" in j:
                level = map_score_to_level(j["score"], boundaries)
                j["satisfaction_level"] = level
                all_satisfaction_levels.append(level)

    # ë§Œì¡±ë„ ì¹´ìš´íŠ¸ ì§‘ê³„
    satisfaction_counts = Counter([level_map.get(level, "ë³´í†µ") for level in all_satisfaction_levels])
    avg_satisfaction = np.mean(all_satisfaction_levels) if all_satisfaction_levels else 3.0

    # LLMì„ ì‚¬ìš©í•œ ë¶„í¬ í•´ì„ ìƒì„± (íŠ¸ë Œë“œ ë©”íŠ¸ë¦­ ê³„ì‚° í›„ì— ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì—¬ê¸°ì„œëŠ” ì´ˆê¸°í™”ë§Œ)
    distribution_interpretation = ""

    # blog_results_list ì—…ë°ì´íŠ¸ (ë§Œì¡±ë„ ë ˆë²¨ í¬í•¨)
    for i, blog_result in enumerate(blog_results_list):
        if i < len(blog_judgments_list):
            judgments = blog_judgments_list[i]
            # ë¬¸ì¥ ìš”ì•½ì— ë§Œì¡±ë„ ë ˆë²¨ ì¶”ê°€
            blog_result["ê¸/ë¶€ì • ë¬¸ì¥ ìš”ì•½"] = "\n---\n".join([
                f"[{res['final_verdict']}({res.get('satisfaction_level', 3)}ì )] {res['sentence']}"
                for res in judgments
            ])
            # í‰ê·  ë§Œì¡±ë„ ì¶”ê°€
            blog_satisfaction_levels = [j.get("satisfaction_level", 3) for j in judgments]
            blog_result["í‰ê·  ë§Œì¡±ë„"] = f"{np.mean(blog_satisfaction_levels):.2f} / 5" if blog_satisfaction_levels else "N/A"

    total_sentiment_frequency = total_pos + total_neg
    total_sentiment_score = ((total_strong_pos - total_strong_neg) / total_sentiment_frequency * 50 + 50) if total_sentiment_frequency > 0 else 50.0

    # ì§€ë‚œ 1ë…„ íŠ¸ë Œë“œ ê·¸ë˜í”„
    trend_graph_path, trend_df = create_trend_graph(keyword, festival_start_date=start_date, festival_end_date=end_date)

    # ì§‘ì¤‘ íŠ¸ë Œë“œ ê·¸ë˜í”„ (ì¶•ì œ ê¸°ê°„ì´ ì—†ì–´ë„ ìµœê·¼ 60ì¼ íŠ¸ë Œë“œ ìƒì„±)
    print(f"ğŸ” '{keyword}' ì§‘ì¤‘ íŠ¸ë Œë“œ ê·¸ë˜í”„ ìƒì„± ì‹œì‘...")
    focused_trend_graph_path, focused_trend_df = create_focused_trend_graph(keyword, start_date, end_date)

    if focused_trend_graph_path:
        print(f"âœ… '{keyword}' ì§‘ì¤‘ íŠ¸ë Œë“œ ê·¸ë˜í”„ ìƒì„± ì„±ê³µ")
    else:
        print(f"âš ï¸ '{keyword}' ì§‘ì¤‘ íŠ¸ë Œë“œ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨")

    trend_metrics = calculate_trend_metrics(trend_df, start_date, end_date)
    # satisfaction_delta: íŠ¸ë Œë“œ ëŒ€ë¹„ ê°ì„± ì¦ê° ë¹„ìœ¨
    # (ê°ì„±ì ìˆ˜ - ì¤‘ë¦½ê°’) / íŠ¸ë Œë“œì§€ìˆ˜ * 100ìœ¼ë¡œ ê³„ì‚°
    trend_index = trend_metrics.get("trend_index", 0)
    satisfaction_delta = ((total_sentiment_score - 50) / (trend_index + 1e-6)) * 100 if trend_index > 0 else (total_sentiment_score - 50)

    # LLMì„ ì‚¬ìš©í•œ ì¢…í•© ë¶„í¬ í•´ì„ ìƒì„± (6ê°œ ì°¨íŠ¸ ëª¨ë‘ í¬í•¨)
    if all_satisfaction_levels:
        try:
            distribution_interpretation = generate_distribution_interpretation(
                satisfaction_counts, len(all_satisfaction_levels), boundaries, avg_satisfaction,
                all_scores=all_scores, outliers=outliers, total_pos=total_pos, total_neg=total_neg,
                trend_metrics=trend_metrics
            )
        except Exception as e:
            print(f"ë§Œì¡±ë„ ë¶„í¬ í•´ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            distribution_interpretation = f"í‰ê·  ë§Œì¡±ë„: {avg_satisfaction:.2f} / 5.0"

    emotion_keyword_freq = Counter(emotion_keywords)

    # ê³„ì ˆë³„ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    seasonal_word_clouds = {"ë´„": {}, "ì—¬ë¦„": {}, "ê°€ì„": {}, "ê²¨ìš¸": {}}
    SEASON_MASK_MAP = {
        "ë´„": "spring",
        "ì—¬ë¦„": "summer",
        "ê°€ì„": "fall",
        "ê²¨ìš¸": "winter"
    }
    for season, pairs in seasonal_aspect_pairs.items():
        if season == "ì •ë³´ì—†ìŒ" or not pairs:
            continue
        
        season_en = SEASON_MASK_MAP.get(season)
        mask_path = None
        if season_en:
            mask_path = os.path.abspath(os.path.join("assets", f"mask_{season_en}.png"))

        pos_wc_path, neg_wc_path = create_sentiment_wordclouds(pairs, f"{keyword}_{season}", mask_path=mask_path)
        
        if pos_wc_path:
            seasonal_word_clouds[season]['positive'] = f"/images/{os.path.basename(pos_wc_path)}"
        if neg_wc_path:
            seasonal_word_clouds[season]['negative'] = f"/images/{os.path.basename(neg_wc_path)}"

    # URL ê²½ë¡œë¡œ ë³€í™˜
    trend_graph_url = f"/images/{os.path.basename(trend_graph_path)}" if trend_graph_path else None
    focused_trend_graph_url = f"/images/{os.path.basename(focused_trend_graph_path)}" if focused_trend_graph_path else None

    # ìµœì¢… ìš”ì•½ ìƒì„±
    negative_summary = summarize_negative_feedback(all_negative_sentences)
    
    # ì„ì‹œ ê²°ê³¼ ê°ì²´ ìƒì„± (ì „ì²´ ìš”ì•½ ìƒì„±ì— í•„ìš”)
    temp_results = {
        "keyword": keyword,
        "avg_satisfaction": avg_satisfaction,
        "total_pos": total_pos,
        "total_neg": total_neg,
        "trend_metrics": trend_metrics,
        "distribution_interpretation": distribution_interpretation,
        "negative_summary": negative_summary
    }
    overall_summary = generate_overall_summary(temp_results)

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    results = {
        "status": f"ì´ {total_searched}ê°œ ê²€ìƒ‰, {len(candidate_blogs)}ê°œ í›„ë³´ ì¤‘ {len(valid_blogs_data)}ê°œ ë¸”ë¡œê·¸ ìµœì¢… ë¶„ì„ ì™„ë£Œ.",
        "total_pos": total_pos, "total_neg": total_neg, "total_strong_pos": total_strong_pos, "total_strong_neg": total_strong_neg,
        "total_sentiment_frequency": total_sentiment_frequency, "total_sentiment_score": total_sentiment_score,
        "seasonal_data": seasonal_data,
        "seasonal_word_clouds": seasonal_word_clouds,
        "negative_summary": negative_summary, # ìš”ì•½ëœ ë‚´ìš©ìœ¼ë¡œ êµì²´
        "overall_summary": overall_summary, # ì¢…í•© í‰ê°€ ì¶”ê°€
        "blog_results_df": pd.DataFrame(blog_results_list) if blog_results_list else pd.DataFrame(),
        "blog_judgments": blog_judgments_list,
        "url_markdown": f"### ë¶„ì„ëœ ë¸”ë¡œê·¸ URL ({len(valid_blogs_data)}ê°œ)\n" + "\n".join([f"- [{b['title']}]({b['link']})" for b in valid_blogs_data]),
        "trend_graph": trend_graph_url,
        "focused_trend_graph": focused_trend_graph_url,
        "trend_df": trend_df,
        "focused_trend_df": focused_trend_df,
        "addr1": addr1,
        "addr2": addr2,
        "areaCode": areaCode,
        "event_period": event_period,
        "festival_start_date": start_date,
        "festival_end_date": end_date,
        "trend_metrics": trend_metrics,
        "satisfaction_delta": satisfaction_delta,
        "emotion_keyword_freq": dict(emotion_keyword_freq.most_common(20)), # ìƒìœ„ 20ê°œë§Œ
        # ë§Œì¡±ë„ ë¶„ì„ ë°ì´í„° ì¶”ê°€
        "all_scores": all_scores,
        "satisfaction_counts": dict(satisfaction_counts),
        "avg_satisfaction": avg_satisfaction,
        "distribution_interpretation": distribution_interpretation,
        "satisfaction_boundaries": boundaries,
        "outliers": outliers,
        "seasonal_texts": {k: "\n".join(v) for k, v in seasonal_texts.items()},
        "seasonal_aspect_pairs": seasonal_aspect_pairs,
        "negative_sentences": all_negative_sentences  # ë¶€ì • ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë„ ìºì‹œì— í¬í•¨
    }

    # ìºì‹œì— ì €ì¥
    save_raw_analysis_to_cache(keyword, num_reviews, results)

    return results

# í•µì‹¬ ë¶„ì„ ë¡œì§ì„ ë‹´ëŠ” ìƒˆ í•¨ìˆ˜
from ..infrastructure.reporting.wordclouds import create_seasonal_trend_wordcloud

def perform_festival_group_analysis(festivals_to_analyze: list, group_name: str, num_reviews: int, driver, log_details: bool, progress: gr.Progress, initial_progress: float, total_steps: int):
    if not festivals_to_analyze: return {"error": f"'{group_name}' ê·¸ë£¹ì—ì„œ ë¶„ì„í•  ì¶•ì œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    category_results, all_blog_posts_list = [], []
    festival_full_results = []
    agg_pos, agg_neg = 0, 0
    agg_strong_pos, agg_strong_neg = 0, 0
    agg_seasonal = {"ë´„": {"pos": 0, "neg": 0}, "ì—¬ë¦„": {"pos": 0, "neg": 0}, "ê°€ì„": {"pos": 0, "neg": 0}, "ê²¨ìš¸": {"pos": 0, "neg": 0}, "ì •ë³´ì—†ìŒ": {"pos": 0, "neg": 0}}
    agg_negative_sentences = []
    agg_seasonal_texts = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
    agg_seasonal_aspect_pairs = {"ë´„": [], "ì—¬ë¦„": [], "ê°€ì„": [], "ê²¨ìš¸": [], "ì •ë³´ì—†ìŒ": []}
    seasonal_trend_scores = {"ë´„": {}, "ì—¬ë¦„": {}, "ê°€ì„": {}, "ê²¨ìš¸": {}}
    total_festivals_sentiment_score = 0
    analyzed_festivals_count = 0

    total_festivals = len(festivals_to_analyze)
    if os.environ.get("LOG_DEBUG") == "true":
        print(f"[DEBUG][CategoryAnalysis] Initial total_festivals: {total_festivals}")

    for i, festival_name in enumerate(festivals_to_analyze):
        def nested_progress_callback(p, desc=""):
            progress(initial_progress + (i + p) / total_festivals / total_steps, desc=f"ë¶„ì„ ì¤‘: {festival_name} ({i+1}/{total_festivals}) - {desc}")

        result = analyze_single_keyword_fully(festival_name, num_reviews, driver, log_details, nested_progress_callback, "ê·¸ë£¹ ë¶„ì„")
        
        if "error" in result or result.get("blog_results_df", pd.DataFrame()).empty:
            print(f"   [{festival_name}] ë¶„ì„ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ.")
            continue

        # ë¶€ì • ë¬¸ì¥ ìš”ì•½ ìƒì„±
        negative_sentences_list = result.get("negative_sentences", [])
        negative_summary = summarize_negative_feedback(negative_sentences_list)

        # ë¶€ì • íŒì •ì€ ìˆì§€ë§Œ ìš”ì•½ì´ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if not negative_summary and result.get("total_neg", 0) > 0:
            negative_summary = f"ë¶€ì • íŒì • {result.get('total_neg', 0)}ê±´ì´ ìˆìœ¼ë‚˜ êµ¬ì²´ì ì¸ ë¶ˆë§Œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¶ˆëª…í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"

        result['precomputed_negative_summary'] = negative_summary
        festival_full_results.append(result)

        # ì‹œì¦Œë³„ íŠ¸ë Œë“œ ì ìˆ˜ ì§‘ê³„
        festival_start_date = result.get("festival_start_date")
        trend_df = result.get("trend_df")
        if festival_start_date and not trend_df.empty:
            season = get_season(festival_start_date.strftime("%Y%m%d"))
            if season != "ì •ë³´ì—†ìŒ":
                peak_trend_score = trend_df['ratio'].max()
                seasonal_trend_scores[season][festival_name] = peak_trend_score

        agg_pos += result.get("total_pos", 0)
        agg_neg += result.get("total_neg", 0)
        agg_strong_pos += result.get("total_strong_pos", 0)
        agg_strong_neg += result.get("total_strong_neg", 0)
        agg_negative_sentences.extend(result.get("negative_sentences", []))
        
        for season, pairs in result.get("seasonal_aspect_pairs", {}).items():
            if pairs:
                agg_seasonal_aspect_pairs[season].extend(pairs)

        for season, texts in result.get("seasonal_texts", {}).items():
            if texts: agg_seasonal_texts[season].append(texts)
        for season, data in result.get("seasonal_data", {}).items():
            agg_seasonal[season]["pos"] += data.get("pos", 0)
            agg_seasonal[season]["neg"] += data.get("neg", 0)
        
        total_freq = result.get("total_sentiment_frequency", 0)
        sentiment_score = result.get('total_sentiment_score', 50.0)
        pos_perc = (result.get('total_pos', 0) / total_freq * 100) if total_freq > 0 else 0.0
        neg_perc = (result.get('total_neg', 0) / total_freq * 100) if total_freq > 0 else 0.0
        
        total_festivals_sentiment_score += sentiment_score
        analyzed_festivals_count += 1
        
        trend_metrics = result.get("trend_metrics", {})
        
        category_results.append({
            "ì¶•ì œëª…": festival_name,
            'ê°ì„± ë¹ˆë„': total_freq,
            'ê°ì„± ì ìˆ˜': f"{sentiment_score:.1f}",
            "ê¸ì • ë¬¸ì¥ ìˆ˜": result.get("total_pos", 0),
            "ë¶€ì • ë¬¸ì¥ ìˆ˜": result.get("total_neg", 0),
            "ê¸ì • ë¹„ìœ¨ (%)": f"{pos_perc:.1f}",
            "ë¶€ì • ë¹„ìœ¨ (%)": f"{neg_perc:.1f}",
            'ì¶•ì œ ê¸°ê°„ (ì¼)': result.get('event_period', 'N/A'),
            'íŠ¸ë Œë“œ ì§€ìˆ˜ (%)': trend_metrics.get('trend_index', 'N/A'),
            'ë§Œì¡±ë„ ë³€í™”': f"{result.get('satisfaction_delta', 0):.1f}",
            'ì£¼ìš” ê°ì„± í‚¤ì›Œë“œ': str(result.get('emotion_keyword_freq', {})),
            'ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ ìš”ì•½': result['precomputed_negative_summary'] or "ì—†ìŒ"
        })

        if "blog_results_df" in result and not result["blog_results_df"].empty:
            all_blog_posts_list.append(result["blog_results_df"])

    if not category_results: 
        return {"error": f"ì„ íƒëœ ê·¸ë£¹ '{group_name}' ë‚´ ëª¨ë“  ì¶•ì œ({total_festivals}ê°œ)ì—ì„œ ìœ íš¨í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

    # ê³„ì ˆë³„ íŠ¸ë Œë“œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    seasonal_trend_wc_paths = {}
    for season, scores in seasonal_trend_scores.items():
        if scores:
            mask_path = os.path.abspath(os.path.join("assets", f"mask_{SEASON_EN_MAP.get(season)}.png"))
            wc_path = create_seasonal_trend_wordcloud(scores, f"{group_name}_{season}_íŠ¸ë Œë“œ", mask_path)
            seasonal_trend_wc_paths[season] = wc_path

    total_sentiment_frequency = agg_pos + agg_neg
    total_sentiment_score = ((agg_strong_pos - agg_strong_neg) / total_sentiment_frequency * 50 + 50) if total_sentiment_frequency > 0 else 50.0
    theme_sentiment_avg = total_festivals_sentiment_score / analyzed_festivals_count if analyzed_festivals_count > 0 else 0.0

    final_category_df = pd.DataFrame(category_results)
    final_all_blogs_df = pd.concat(all_blog_posts_list, ignore_index=True) if all_blog_posts_list else pd.DataFrame()

    # ì¹´í…Œê³ ë¦¬ ê³„ì ˆë³„ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    category_seasonal_word_clouds = {"ë´„": {}, "ì—¬ë¦„": {}, "ê°€ì„": {}, "ê²¨ìš¸": {}}
    SEASON_MASK_MAP = {
        "ë´„": "spring",
        "ì—¬ë¦„": "summer",
        "ê°€ì„": "fall",
        "ê²¨ìš¸": "winter"
    }
    for season, pairs in agg_seasonal_aspect_pairs.items():
        if season == "ì •ë³´ì—†ìŒ" or not pairs:
            continue

        season_en = SEASON_MASK_MAP.get(season)
        mask_path = None
        if season_en:
            mask_path = os.path.abspath(os.path.join("assets", f"mask_{season_en}.png"))

        pos_wc_path, neg_wc_path = create_sentiment_wordclouds(pairs, f"{group_name}_{season}", mask_path=mask_path)

        if pos_wc_path:
            category_seasonal_word_clouds[season]['positive'] = f"/images/{os.path.basename(pos_wc_path)}"
        if neg_wc_path:
            category_seasonal_word_clouds[season]['negative'] = f"/images/{os.path.basename(neg_wc_path)}"

    # ì¹´í…Œê³ ë¦¬ ì¢…í•© í‰ê°€ ìƒì„±
    category_negative_summary = ""
    category_overall_summary = ""

    if agg_negative_sentences:
        category_negative_summary = summarize_negative_feedback(agg_negative_sentences)

    # ì¢…í•© í‰ê°€ìš© ì„ì‹œ ê²°ê³¼ ê°ì²´
    if analyzed_festivals_count > 0:
        category_temp_results = {
            "keyword": group_name,
            "total_pos": agg_pos,
            "total_neg": agg_neg,
            "trend_metrics": {"trend_index": 0},  # ì¹´í…Œê³ ë¦¬ì—ëŠ” íŠ¸ë Œë“œ ì§€í‘œê°€ ì—†ìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •
            "distribution_interpretation": f"{group_name} ì¹´í…Œê³ ë¦¬ ë‚´ {analyzed_festivals_count}ê°œ ì¶•ì œë¥¼ ì¢…í•© ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.",
            "negative_summary": category_negative_summary
        }
        category_overall_summary = generate_overall_summary(category_temp_results)

    if os.environ.get("LOG_DEBUG") == "true":
        print(f"[DEBUG][CategoryAnalysis] Final total_festivals: {total_festivals}, analyzed_festivals_count: {analyzed_festivals_count}")

    return {
        "status": f"ì´ {total_festivals}ê°œ ì¶•ì œ ìš”ì²­ ì¤‘ {len(category_results)}ê°œ ë¶„ì„ ì™„ë£Œ.",
        "total_festivals": total_festivals,
        "analyzed_festivals": analyzed_festivals_count,
        "total_pos": agg_pos, "total_neg": agg_neg,
        "total_sentiment_frequency": total_sentiment_frequency,
        "total_sentiment_score": total_sentiment_score,
        "theme_sentiment_avg": theme_sentiment_avg,
        "seasonal_data": agg_seasonal,
        "negative_sentences": agg_negative_sentences,
        "seasonal_texts": {k: "\n".join(v) for k, v in agg_seasonal_texts.items()},
        "seasonal_aspect_pairs": agg_seasonal_aspect_pairs,
        "seasonal_trend_wc_paths": seasonal_trend_wc_paths, # ì›Œë“œí´ë¼ìš°ë“œ ê²½ë¡œ ì¶”ê°€
        "individual_festival_results_df": final_category_df,
        "all_blog_posts_df": final_all_blogs_df,
        "festival_full_results": festival_full_results,
        "all_blog_judgments": [j for res in festival_full_results for j in res.get("blog_judgments", [])],
        "category_overall_summary": category_overall_summary,  # ì¹´í…Œê³ ë¦¬ ì¢…í•© í‰ê°€
        "category_negative_summary": category_negative_summary,  # ì¹´í…Œê³ ë¦¬ ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­
        "category_seasonal_word_clouds": category_seasonal_word_clouds  # ì¹´í…Œê³ ë¦¬ ê³„ì ˆë³„ ì›Œë“œí´ë¼ìš°ë“œ
    }

# ê¸°ì¡´ í•¨ìˆ˜ëŠ” ìƒˆë¡œ ë§Œë“  ê·¸ë£¹ ë¶„ì„ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ë˜í¼(wrapper)ê°€ ë¨
def perform_category_analysis(cat1, cat2, cat3, num_reviews, driver, log_details, progress: gr.Progress, initial_progress, total_steps):
    festivals_to_analyze = festival_loader.get_festivals(cat1, cat2, cat3)
    category_name = cat3 or cat2 or cat1
    return perform_festival_group_analysis(festivals_to_analyze, category_name, num_reviews, driver, log_details, progress, initial_progress, total_steps)